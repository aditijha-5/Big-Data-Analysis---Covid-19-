# ğŸ¦  Big Data Analysis of COVID-19 Using PySpark

This project demonstrates how to perform scalable data analysis on a real-world COVID-19 dataset using Apache **PySpark** in **Google Colab**. It focuses on extracting insights such as top affected provinces and cities, handling missing data, and exploring basic correlations, all while leveraging PySpark's distributed data processing capabilities.

---

## ğŸ“Œ Project Objective

To showcase how PySpark can be used to process and analyze large-scale datasets efficiently by:

- Aggregating COVID-19 cases by location (province/city)
- Handling missing values in big data pipelines
- Performing statistical correlation analysis
- Demonstrating the power of distributed data processing

---

## ğŸ§° Tools & Technologies

- **Python 3**
- **Apache PySpark**
- **Google Colab**
- **Pandas (optional)**
- **CSV data input**

---

## ğŸ“ Dataset Overview

The dataset used contains COVID-19 case data across various cities and provinces.

**Columns include:**
- `case_id` â€“ Unique identifier for each case
- `province` â€“ State/province/region
- `city` â€“ City name
- `infection_case` â€“ Case type (group, visit, overseas, etc.)
- `confirmed` â€“ Number of confirmed cases
- `latitude`, `longitude` â€“ Geographical coordinates

---

## ğŸ” Key Analysis Performed

- âœ… Dropped rows with missing/null values
- âœ… Grouped and summed confirmed cases by `province` and `city`
- âœ… Found top 10 regions with highest number of confirmed cases
- âœ… Calculated correlation between confirmed cases and latitude
- âœ… Aggregated average coordinates and cases for visual/statistical insights

